---
title: "Coursera Practical Machine Learning Class - Project"
author: "Lo√Øc BERTHOU"
date: "August 23, 2015"
output: html_document
---

```{r, echo=FALSE}
set.seed(20150823)
```

## Executive Summary

This document aims at demonstrating how we can use devices such as Jawbone Up, Nike FuelBand, and Fitbit in order to track the quality of exercises instead of quantity like it is the case most of the time. In our specific case, we are trying to predict if an athlete has perform barbell lifts correctly or incorrectly.

## Data 

The training data for this project are available here: 

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

The test data are available here: 

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

The data for this project come from this source: http://groupware.les.inf.puc-rio.br/har.

## Data Cleaning

We will start by loading the _training_ and _testing_ datasets.

```{r, cache=TRUE}
trainingSet = read.csv("pml-training.csv", header = TRUE, stringsAsFactors = FALSE, na.strings = c("", "#DIV/0!", "NA"))
testingSet = read.csv("pml-testing.csv", header = TRUE, stringsAsFactors = FALSE, na.strings = c("", "#DIV/0!", "NA"))
```

Let's have a brief overview of them.

```{r}
str(trainingSet)
str(testingSet)
```

Our initial exploratory data analysis shows that some columns contain N/A values. Let's first count these N/A values.

```{r, cache=TRUE}
isNaMatrixTraining <- apply(trainingSet, 1, is.na)
countNaMatrixTraining <- apply(isNaMatrixTraining, 1, sum)
countNaMatrixTraining <- as.data.frame(countNaMatrixTraining)
countNaMatrixTraining

isNaMatrixTesting <- apply(testingSet, 1, is.na)
countNaMatrixTesting <- apply(isNaMatrixTesting, 1, sum)
countNaMatrixTesting <- as.data.frame(countNaMatrixTesting)
countNaMatrixTesting
```

Since the number of N/A values is non-neglectable, we're going to remove all these columns.

```{r}
# Combine the columns that contain N/A values.
noNaMatrix <- as.data.frame(cbind(countNaMatrixTraining[,1] == 0, countNaMatrixTesting[,1] == 0))
names(noNaMatrix) <- c("training", "testing")
noNaMatrix$combined <- noNaMatrix$training & noNaMatrix$testing

trainingSetClean <- trainingSet[, noNaMatrix$combined]
testingSetClean <- testingSet[, noNaMatrix$combined]
```

We will also remove the timestamp columns that will not be used in the analysis.

```{r}
trainingSetClean <- subset(trainingSetClean, select=-c(X, user_name, raw_timestamp_part_1, raw_timestamp_part_2, cvtd_timestamp, new_window, num_window))
testingSetClean <- subset(testingSetClean, select=-c(X, user_name, raw_timestamp_part_1, raw_timestamp_part_2, cvtd_timestamp, new_window, num_window))
```

Now we will make the factor variables as factor columns.

```{r}
trainingSetClean$classe <- as.factor(trainingSetClean$classe)
```

## Data Modeling

We first load the libraries for machine learning and parallelization (to set-up our system to use multiple cores).

```{r}
library(caret)
library(doParallel)
numcores<-detectCores()
# Create the cluster of workers. It is a good practise to close it when computation is done.
cl <- makeCluster(numcores) 
registerDoParallel(cl)
```

### Sub-Sampling

Since we know that our training set contains many observations (`r nrow(trainingSetClean)` rows), the training algorithm will take a long time. We will then first use a small sample of the original training sample to investigate the best model to use in our machine learning algorithm.

```{r}
# Sub-sampling 500 rows randomly
sample1 <- trainingSetClean[sample(1:nrow(trainingSetClean), 500, replace=FALSE),]

# Creating the training and testing data partitions (75% training)
dataPartitions <- createDataPartition(sample1$classe, p=0.75, list=FALSE)
training1 <- sample1[dataPartitions[, 1], ]
testing1 <- sample1[-dataPartitions[, 1], ]

# Checking the size our new sub datasets
dim(training1)
dim(testing1)
```

### The model is being generated with the Caret package and the 'Random Forest' method.

```{r}
fitRf <- train(classe ~ ., method="rf", data=training1)
print(fitRf$finalModel)
```

Let's predict the values on the sub-testing sample anc compare them to the reference.

```{r}
predictRf <- predict(fitRf, testing1)
confusionMatrix(predictRf, testing1$classe)
```

### The model is being generated with the Caret package and the 'Prediction Tree' method.

```{r}
fitRpart <- train(classe ~ ., method="rpart", data=training1)
print(fitRpart$finalModel)
```

Let's predict the values on the sub-testing sample anc compare them to the reference.

```{r}
predictRpart <- predict(fitRpart, testing1)
confusionMatrix(predictRpart, testing1$classe)
```

### Choosing the model

After trying out a few different models, the **Random Forest** seems like the most accurate. We will then run the training on the whole training set to create the final model used for our prediction.

```{r, cache=TRUE}
fit <- train(classe ~ ., method="rf", data=trainingSetClean)
print(fit$finalModel)

# Good practice to shut down the workers.
stopCluster(cl)
```

Now we can predict the class for the test dataset.

```{r}
testingSetClean$classe <- predict(fit, testingSetClean)
```

We will extract the result data that will be submitted for grading.

```{r}
results <- subset(testingSetClean, select = c(problem_id, classe))
results
```

And generate the result files.

```{r}
pml_write_files = function(x){
    n = length(x)
    for(i in 1:n){
        filename = paste0("problem_id_",i,".txt")
        write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
    }
}
pml_write_files(results$classe)
```

## Appendix

### Appendix 1. Exploratory Code

```{r}
dataPartitions <- createDataPartition(trainingSetClean$classe, times=10, p=0.75, list=FALSE)

training1 <- trainingSetClean[dataPartitions[, 1], ]
testing1 <- trainingSetClean[-dataPartitions[, 1], ]
dim(training1)
dim(testing1)

training1 <- createFolds(trainingSetClean$classe, k=10, returnTrain = TRUE)
testing1 <- createFolds(trainingSetClean$classe, k=10, returnTrain = FALSE)
sapply(training1, length)
sapply(testing1, length)
```
